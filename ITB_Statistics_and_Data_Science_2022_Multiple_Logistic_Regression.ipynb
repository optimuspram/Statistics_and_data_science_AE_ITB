{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ITB_Statistics_and_Data_Science_2022  - Multiple Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/optimuspram/Statistics_and_data_science_AE_ITB/blob/main/ITB_Statistics_and_Data_Science_2022_Multiple_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gkTT4eUHKYo"
      },
      "source": [
        "# **Logistic Regression Tutorial**\n",
        "**Written by: Muhammad Daffa Robani, Faiz Izzaturrahman**\n",
        "\n",
        "**Edited and maintained by: Pramudita Satria Palar, Javensius Sembiring, Edy Suwondo, Khairul Ummah.**\n",
        "\n",
        "**Last updated: 15-April-2022**\n",
        "\n",
        "\n",
        "This Python notebook demonstrates and explains a logistic regression model applied to a simple two-variable problem. \n",
        "\n",
        "There are several existing packages for linear regression (e.g. ```sklearn```) that you can use easily. However, in this notebook, we coded logistic regression from scratch so you can see the translation of what you just learned from class into Python codes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS_iZFUmnsL8"
      },
      "source": [
        "The very first step is to import several packages that we will use, to be more specific:\n",
        "\n",
        "\n",
        "*   ```numpy``` for supporting multi-dimensional matrices.\n",
        "*   ```pandas``` for importing the CSV data (although pandas has more capabilities!)\n",
        "*   ```scipy```, specifically we will use the statistic package from scipy.\n",
        "*   ```statsmodels```, for more statistics related functions\n",
        "*   ```matplotlib``` for plotting the data.\n",
        "*   ```mpl_toolkits``` for 3D plotting.\n",
        "\n",
        "Please start by executing the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jxZTUWDC8F1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from statsmodels.stats import weightstats as stests\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 13})\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import io\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vriiburmH3Un"
      },
      "source": [
        "# Importing and Organizing The Data + Preliminary analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKpfeqB0Qlhr"
      },
      "source": [
        "The very first step that you need to do is to upload the data into your local cloud folder. To that end, we will use ```files.upload()``` from ```google.colab```. For this tutorial, we will use the ```univ_score.csv```. However, you can actually use any CSV/TXT data! The first columns of your CSV should consist of the matrix of observation (your data set $\\boldsymbol{X}$): \n",
        "\n",
        "$\\boldsymbol{X} = \\begin{bmatrix} x_{11} & x_{12} & \\ldots & x_{1p}\\\\\n",
        "x_{21} & x_{22} & \\ldots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        " x_{n1} & x_{n2} & \\ldots & x_{np}\\end{bmatrix}$\n",
        "\n",
        "where $n$ is the number of samples and $p$ is the number of input variables / features. The last column of your CSV should consists of the output $\\boldsymbol{y}$:\n",
        "\n",
        "$\\boldsymbol{y} = ( y_{1},y_{2},\\ldots,y_{n})^{T}$\n",
        "Notice that the label $\\boldsymbol{y}$ is categorical for classification problem.\n",
        "\n",
        "Let us execute the following cell to upload your CSV files into the local folder:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8mwJbDEIAal"
      },
      "source": [
        "from google.colab import files\n",
        "#Upload the data to be modelled\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF5BEHwYIFbF"
      },
      "source": [
        "#Read the data using panda module, data that will be modelled is saved as a variable called data\n",
        "data = pd.read_csv('univ_score.csv',index_col='Unnamed: 0')\n",
        "#Reformat the data as numpy array, to make it consistent with other following type of variables. This variable is saved as data_array.\n",
        "data_array = data.to_numpy()\n",
        "n_entries = data_array.shape[0] #Count the number of data entries\n",
        "n_var = data_array.shape[1]-1 #Count the number of features (variables/predictors)\n",
        "print(\"Five first rows of the data: \")\n",
        "print(data_array[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAZA6AWCIavi"
      },
      "source": [
        "#Split the data into X and y\n",
        "X = data.iloc[:,0:n_var].to_numpy() #Take X from the data\n",
        "y = data.iloc[:,-1].to_numpy() #Take y from the data\n",
        "print(\"Five first rows of X: \")\n",
        "print(X[:5])\n",
        "print(\"Five first rows of y: \")\n",
        "print(y[:5].reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y0rxR-FKm7f"
      },
      "source": [
        "## **Pearson Correlation Coefficient (r)**\n",
        "One useful technique to check the correlation between variables is to use Pearson correlation coefficient. You learned the basic from class, so you know that positive $r$ means that both variables change in the same direction (and the opposite for negative $r$). Here is the equation for your reminder.\n",
        "\n",
        "$r = \\frac{\\sum{(X_1 - \\mu_{X_1})(X_2 - \\mu_{X_2})}}{\\sqrt{\\sum{(X_1 - \\mu_{X_1})^2}\\sum{(X_2 - \\mu_{X_2})^2}}}$\n",
        "\n",
        "The Pearson Correlation Coefficient calculates the linear relationship between two datasets $X_1$ and $X_2$ with their respective mean values $\\mu_{X_1}$ and $\\mu_{X_2}$ . The coefficient, **r**, takes on values between -1 and +1. Whereby the sign of the value indicates negative or positive correlation and the magnitude determines how linear the correlation is. \n",
        "\n",
        "We will make a correlation matrix because you have two input and one output variables. For that, we will call ```stats.pearsonr``` to calculate the correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkD25ohgId26"
      },
      "source": [
        "#Function to create correlation matrix\n",
        "def correlmat(data):\n",
        "    n_var = data.shape[1]-1\n",
        "    correl = np.zeros((n_var+1,n_var+1))\n",
        "    for i in range(n_var+1):\n",
        "        for j in range(n_var+1):\n",
        "            correl[i,j],_ = stats.pearsonr(data[:,i],data[:,j]) #Calculate pearson's correlation coefficient\n",
        "    return correl"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0usPEo3hKvE4"
      },
      "source": [
        "Based on the data we have, we compute the correlation matrix by calling the ``` correlmat ``` function \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m4Qnjz_IfKQ"
      },
      "source": [
        "#Call correlmat function to obtain the correlation matrix\n",
        "correl_matrix = correlmat(data_array)\n",
        "print(\"Correlation Matrix: \")\n",
        "print(correl_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgawz5ylLYUZ"
      },
      "source": [
        "Here we would like to create a function in which we can visualize our samples. As opposed to a regression where our outputs are continuous values, the task of classification refers to their outputs as **labels**. In this scenario we have 2 labels i.e. **2 classes of data, 0-class (y=0) and 1-class (y=1)**.\n",
        "Since our $X$ data has two features, we would expect a 2D plot.\n",
        "\n",
        "First, we split our $X$ array based on their labels $y$. To do so we would first extract the indices of $y$, where $y=1$ and similarly where $y=0$. These indices are stored in the variables ``` pos ``` and ``` neg ``` respectively.\n",
        "In a single figure, we generate two scatter plots from then on. Each corresponds to their labels. As you shall see different colors will denote each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3961mXDInGc",
        "cellView": "form"
      },
      "source": [
        "#@title Visualizing the data with scatter plot\n",
        "#This procedure can only be conducted with 1/2 number of features only\n",
        "#Function to create the scatter plot\n",
        "def datascatterplot(X,y):\n",
        "    pos = np.where(y==1) #To find rows that have y=1\n",
        "    neg = np.where(y==0) #To find rows that have y=0\n",
        "    n_var = X.shape[1]\n",
        "    if n_var==1: #Procedure for one variable\n",
        "        fig = plt.figure(figsize=[8,6])\n",
        "        plt.scatter(X[pos],y[pos])\n",
        "        plt.scatter(X[neg],y[neg])\n",
        "        plt.xlabel('x',fontsize=14)\n",
        "        plt.ylabel('y',fontsize=14)\n",
        "        plt.show()\n",
        "    elif n_var==2: #Procedure for two variables\n",
        "        fig = plt.figure(figsize=[8,6])\n",
        "        plt.scatter(X[pos,0],X[pos,1],label='y=1')\n",
        "        plt.scatter(X[neg,0],X[neg,1],label='y=0')\n",
        "        plt.xlabel('x1',fontsize=14)\n",
        "        plt.ylabel('x2',fontsize=14)\n",
        "        plt.title('Scatter Plot of The Samples',fontsize=16)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Your data has more than two variables\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPwAf71Jdeg"
      },
      "source": [
        "#To create the visualization, call datascatterplot_log\n",
        "datascatterplot(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBi1a8RH4AJ"
      },
      "source": [
        "# Creating a Logistic Regression Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T-M8DynUCEg"
      },
      "source": [
        "###**Concept**\n",
        "As opposed to Linear Regression, whose aim was to generate a model that could predict outputs whose values are continuous, we would like to create a classification model with discrete predictions, e.g. 0 or 1. Normally, the task of predicting which class a point belongs to is deemed a **classification task**. However, with Logistic Regression, we would like to *model the **probability** that a specific output $Y$ belongs to a particular class/category*. In the present scenario, our data consists of samples that belong to either one of two classes, $Y=0$ or  $Y=1$. The specific task that handles data with two classes is called a **binary classification**.\n",
        "\n",
        "With logistic regression, we create a regression model whose predictions represent a probability. Previously with linear regression, the model took the form of $Y = \\beta X$ where $Y$. The logistic regression model takes the form of a logistic/sigmoid function which restricts the values of the outputs to fall in the range of $0-L$, where $L$ is some arbitrary value reflective of the outputs $Y$, usually the maximum, i.e. if $Y \\in [0, 1]$ then $L=1$, so that we can interpret is a probability. A *threshold* at $L/2$ is usually set to denote whether a particular sample is an instance of the *default* class if its probability is above this threshold. This is convenient since the task is to classify our samples according to their classes, represented by a discrete value, but would also provide us with some form of *uncertainty* in our predictions.\n",
        "\n",
        "For example, let the samples $X$ whose $Y=1$ denote the *default* class $C1$, as opposed to the class $C0$ where $Y=0$. Given a test sample $x$, if the model predicts its probability $p(x)$ as above $1/2$ then we are *certain* that $x \\in C1$, otherwise if it is less than $1/2$ then $x \\in C0$\n",
        "\n",
        "Furthermore, since this is a regression model, we would have to *train* the model by finding the best coefficients, $\\beta$, to represent the model. Like with linear regression, these coefficients are multiplied with the samples $X$.\n",
        "\n",
        "From here on, we can now move on to the details\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62lT39THPYAn"
      },
      "source": [
        "### **Logistic/Sigmoid Function**\n",
        "\n",
        "![logistic.png](data:image/png;base64,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)\n",
        "\n",
        "Logistic regression models the probability of a default class, say for the samples $X$ whose $Y=1$. Previously, we mentioned that the model takes the form of a logistic/sigmoid function. The general formula is written as,\n",
        "$$p(X)=  \\frac{L}{1+e^{-(\\beta X)} }$$\n",
        "However, since we would like to model to predict values between 0-1, we set $L=1$ and automatically $p(X)$ will be a fraction of 1 and positive since the denominator is restricted by the negative exponential, i.e. $e^{-9999} = 0$. This is written in the following ``` sigmoid ``` function (please execute this cell):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr8uc-raJqsz"
      },
      "source": [
        "#Function to call sigmoid based on scores\n",
        "def sigmoid(scores):\n",
        "    return 1/(1+np.exp(-scores))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUdRyaY0PbsI"
      },
      "source": [
        "### **Model Prediction**\n",
        "Just before we get into the training procedure, we look as to how the model makes predictions. In the ```predict``` function, we first **concatenate** our $X$ data with a vector of ones to get an array $F$. As with linear regression, this is simply representative of the *intercept* i.e. $\\beta_0$. Thus, the variable ```y_pred``` is $p(X)$ and  ``` scores ``` represents $F\\beta$ to which the predictions at new test points is simply the evaluation of the ```sigmoid``` function with $F\\beta$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEeTRgiMdKQz"
      },
      "source": [
        "#Function to calculate scores and/or prediction given by a logistic regression model\n",
        "def predict(X,beta):\n",
        "  #Create F matrix\n",
        "  F = np.ones((X.shape[0],X.shape[1]+1))\n",
        "  F[:,1:] = X\n",
        "  #Calculate prediction\n",
        "  scores = F@beta\n",
        "  y_pred = sigmoid(scores)\n",
        "  return scores, y_pred"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LvlaCfKTUN6"
      },
      "source": [
        "### **Training the Model**\n",
        "To train the logistic regression model we use the **maximum likelihood principle**. The principle is essentially to **maximize the likelihood function**. The likelihood function, is **a function that measures the goodness of fit between a model and the given (training) data**. The parameters that control the model is of course $\\beta$. Thus, the likelihood is a function of $\\beta$ and for the default class $C1$ is written as,\n",
        "\n",
        "$$ L(\\beta) = \\prod_{x \\in C1}{p(x)} \\prod_{x' \\in C0}{(1-p(x'))} $$\n",
        "\n",
        "For reasons outside the scope of this course, we shall instead consider maximizing the **log likelihood function** which is mathematically equivalent but just written differently for ease of computation. For a total of $N$ samples in the data array $X$, this is written as, \n",
        "\n",
        "$$ \\text{log}(L(\\beta)) = \\sum_{i=1}^{N} (y_i \\beta x_i - \\text{log}(1+e^{\\beta x_i}))$$\n",
        "\n",
        "This function is written in ```log_likelihood_cal ```:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50U1nCIgJxgH"
      },
      "source": [
        "#Function to calculate log likelihood\n",
        "def log_likelihood_cal(X,y,beta):\n",
        "    #Calculate scores at every value of X\n",
        "    scores, _ = predict(X, beta)\n",
        "    log_likelihood = np.sum(y*scores-np.log(1+np.exp(scores))) #Calculate log likelihood\n",
        "    return log_likelihood"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kupWDeFLwuS2"
      },
      "source": [
        "Recall your days of multivariable calculus, specifically the topic of **finding optima** in functions. That is, the problem of **finding stationary points** (zero first derivatives). Computationally, this is done through an iterative procedure. The specific algorithm to be used in the present work is the **Newton-Raphson algorithm**.\n",
        "\n",
        "\n",
        "Since we are trying to find stationary points in the log-likelihood function then we would need the first derivative.\n",
        "\n",
        "$$ \\frac{\\partial L(\\beta)}{\\partial \\beta} = \\sum_{i=1}^{N} F_i (y_i - p(x_i, \\beta))$$\n",
        "\n",
        "(Recall that $F$ is simply $F = [1^T X]$ and $p$ is the evaluation of the ```sigmoid``` function with the current $F\\beta$ values.)\n",
        "\n",
        "Furthermore, with the **Newton-Raphson algorithm** we would require the second-derivative/Hessian matrix.\n",
        "$$ \\frac{\\partial^2 L(\\beta)}{\\partial \\beta \\partial \\beta^T} = \\sum_{i=1}^{N} -F_i F_i^{T}p(x_i, \\beta) (1 - p(x_i, \\beta))$$\n",
        "\n",
        "Please execute the cell below to create the ```log_likelihood_der_cal``` subroutine that we will need to find the best logistic regression model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Anboi3uKd5f"
      },
      "source": [
        "#Function to call the derivative of log likelihood calculation\n",
        "def log_likelihood_der_cal(X,y,beta):\n",
        "    #Create F matrix\n",
        "    F = np.ones((X.shape[0],X.shape[1]+1))\n",
        "    F[:,1:] = X\n",
        "    #Calculate scores\n",
        "    scores,_ = predict(X, beta)\n",
        "    #Calculate the first derivatives\n",
        "    dl = np.sum(F*(y-sigmoid(scores)).reshape(-1,1),axis=0) #First derivatives\n",
        "\n",
        "    #Calculate the second derivatives\n",
        "    temp = np.ones((F.shape[1],F.shape[1],F.shape[0]))\n",
        "    for i in range(X.shape[0]):\n",
        "        temp[:,:,i] = np.multiply(F[i,:].reshape(-1,1)@F[i,:].reshape(-1,1).T,-1*sigmoid(scores[i])*(1-sigmoid(scores[i])))\n",
        "    dl2 = np.sum(temp,axis=2) #Second derivatives\n",
        "    return dl,dl2"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7iTw0oNwgJt"
      },
      "source": [
        "The process of maximizing the log-likelihood, would go as follows:\n",
        "\n",
        "**Initialization:**\n",
        "\n",
        "1. Set initial $\\beta_0$ to, say, all zeros. ($\\beta^{old}=\\beta_0$)\n",
        "2. Set an initial log-likelihood to zero $(LL^{old})$, and compute another log-likelihood $(LL^{now})$ based on $\\beta_0$ \n",
        "3. Set a tolerance value, say $1\\times 10^{-6}$ , which shall form the stopping criteria as $ if \\quad (|LL^{now} - LL^{old}| < 1\\times 10^{-6}) \\rightarrow stop$\n",
        "\n",
        "**Main Procedure**\n",
        "\n",
        "$while \\quad |LL^{now} - LL^{old}| > 1\\times 10^{-6}$\n",
        "1. Compute $\\beta^{now} = \\beta^{old} - \\Big(\\frac{\\partial^2 L(\\beta)}{\\partial \\beta \\partial \\beta^T}\\Big)^{-1}\\Big(\\frac{\\partial L(\\beta)}{\\partial \\beta}\\Big)$\n",
        "2. Set $LL^{old} = LL^{now}$\n",
        "3. Recompute $LL^{now}$ based on $\\beta^{now}$\n",
        "3. Recompute $\\frac{\\partial^2 L(\\beta)}{\\partial \\beta \\partial \\beta^T}$ and $\\frac{\\partial L(\\beta)}{\\partial \\beta}$\n",
        "\n",
        "The cell below contains the subroutine that we will use to to find the best logistic regression parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz6p9UQuK6cC"
      },
      "source": [
        "#Function to create a logistic regression model on the data. The model is obtained by conducting procedure to maximize log likelihood.\n",
        "def logistic_regression_train(X,y):\n",
        "    #Initial values for the model coefficients. (All set to zeros)\n",
        "    beta = np.zeros((X.shape[1]+1))\n",
        "    \n",
        "    likelihood_old = 0\n",
        "    likelihood_now = log_likelihood_cal(X,y,beta) #Log likelihood value of the initial model\n",
        "    [dl,dl2] = log_likelihood_der_cal(X,y,beta) #Log likelihood derivatives of the initial model \n",
        "    it = 0\n",
        "    tol = 1e-6 #Tolerance value to stop iteration\n",
        "    while np.abs(likelihood_now-likelihood_old) > tol:\n",
        "        it = it+1\n",
        "        beta = beta-np.linalg.inv(dl2)@dl #New model coefficients, based on a gradient descent optimization scheme.\n",
        "        likelihood_old = np.copy(likelihood_now) #Log likelihood value of the previous model.\n",
        "        likelihood_now = log_likelihood_cal(X,y,beta) #Log likelihood value of the current model.\n",
        "        [dl,dl2] = log_likelihood_der_cal(X,y,beta) #Log likelihood derivatives of the current model\n",
        "        print(\"Iteration: {}, Likelihood:{}\".format(it,likelihood_now))\n",
        "        \n",
        "    se = np.sqrt(np.diag(np.linalg.inv(-dl2))) #Calculate standard deviation of each model coefficient \n",
        "    return beta,likelihood_now, se"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsBoi7EYjjCD"
      },
      "source": [
        "The following cell train your logistic regression model based on the given $X$ and $y$. The procedure automatically stops when it finds the optimum parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iar0_g3xMfRX"
      },
      "source": [
        "#To obtain a logistic regression model on your data, call logistic_regression_train\n",
        "[beta,likelihood,beta_se] = logistic_regression_train(X,y)\n",
        "\n",
        "# Print the coefficients\n",
        "print(\"The coefficients of your logistic regression model are as follows:\")\n",
        "for i in range(0,n_var+1):\n",
        "    print('beta_{} = {:.4f}'.format(i,beta[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFuGv_7zH7-C"
      },
      "source": [
        "# 3. Evaluating and Interpreting The Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl-QEoeQ16cB"
      },
      "source": [
        "After you build your logistic regression model, now you want to assess its performance and accuracy. To that end, you would need the model parameter statistics that summarize important values. Here we create a summary for the model parameter statistics:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCxmP6RVMsOQ"
      },
      "source": [
        "#Function to create summary matrix\n",
        "def summarymat_log(beta, beta_se):\n",
        "    n_var = beta.shape[0]-1\n",
        "    summary = np.zeros((n_var+1,5))\n",
        "    summary[:,0] = beta #Model coefficients. \n",
        "    summary[:,1] = beta_se #Standard deviation of each model coefficient. \n",
        "    summary[:,2] = np.divide(summary[:,0],summary[:,1]) #Z-statistic.\n",
        "    summary[:,3] = stats.norm.sf(np.abs(summary[:,2]))*2 #Calculate p-value based on z-statistic.\n",
        "    #Hypothesis test with 95% confidence interval\n",
        "    for i in range(n_var+1):\n",
        "        if summary[i,3]<= 0.05: #If p<=0.05, null hypothesis is denied\n",
        "            summary[i,4] = 1\n",
        "        else:\n",
        "            summary[i,4] = 0 #If p<=0.05, null hypothesis is accepted\n",
        "    return summary"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53vqzGI_Muc4"
      },
      "source": [
        "#To create summary matrix of the trained model, call summarymat_log function\n",
        "summary = summarymat_log(beta, beta_se)\n",
        "print(\"Summary matrix: \")\n",
        "print(pd.DataFrame(summary,columns=['Beta','Standard Deviation', 'z-statistic', 'p-value', 'Hypothesis Test']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzoSG6Z82c3z"
      },
      "source": [
        "As with linear regression previously, we can compute the $R^{2}$ score to see how accurate our regression model is applicable on continuous outputs. However, since our outputs will ultimately be categorical, we consider a *pseudo* $R^{2}$ metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fartOjjZcYqT"
      },
      "source": [
        "def pseudo_r2_comp(y_true,y_pred):\n",
        "    pos = np.where(y_true==1) #To determine which rows with y=1\n",
        "    neg = np.where(y_true==0) #To determine which rows with y=0\n",
        "    #Calculate odds\n",
        "    odds = pos[0].shape[0]/neg[0].shape[0]\n",
        "    #Procedure to calculate pseudo TSS\n",
        "    p_odds = odds/(1+odds)\n",
        "    pseudo_tss = np.sum(y_true*np.log(p_odds)+(1-y_true)*np.log(1-p_odds))\n",
        "    #Procedure to calculate pseudo R2\n",
        "    #Calculate scores\n",
        "    scores = np.log(y_pred/(1-y_pred))\n",
        "    log_likelihood = np.sum(y_true*scores-np.log(1+np.exp(scores))) #Calculate log likelihood\n",
        "    pseudo_R2 = (pseudo_tss-likelihood)/pseudo_tss\n",
        "    return pseudo_R2"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSovSJFBNRs-"
      },
      "source": [
        "#To obtain pseudo R2 error, call pseudo_R2_comp function\n",
        "_, y_pred = predict(X, beta)\n",
        "pseudo_R2 = pseudo_r2_comp(y,y_pred)\n",
        "print(\"Pseudo R2 error: {}\".format(pseudo_R2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following cell to predict the probability of an input belonging to class 1 at any location:"
      ],
      "metadata": {
        "id": "dpl-s_CCDxHN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B88DMTXZsna5"
      },
      "source": [
        "_, y_pred = predict(np.array([[80,60],[20, 20],[50, 50]]), beta)\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RTMIpYE699R"
      },
      "source": [
        "Here we visualize the results of our model with predictions made along a $X$ space. Previously our ```predict``` function gave us the probabilities of an output, however, since the aim here is to classify samples we introduce the extra steps of including the *threshold* as previously mentioned. That is included in the snippet from the code below,\n",
        "\n",
        "```\n",
        "_,Z = predict(np.c_[xx.ravel(), yy.ravel()], beta)\n",
        "cond_list = [Z<0.5,Z>=0.5] \n",
        "choice_list = [0, 1] # Introduce a list of variables in which our predictions can take, that is 0 or 1\n",
        "Z = np.select(cond_list,choice_list) # \n",
        "```\n",
        "where ```cond_list``` seperates the predicted samples Z corresponding to their probabilities and we introduce a variable```choice_list``` where the values in the list correspond to the condition in ```cond_list```.\n",
        "\n",
        "The values in the ```Z``` array is then changed accordingly.\n",
        "\n",
        "$Z<0.5 \\rightarrow Z=0$ \n",
        "\n",
        "$Z>0.5 \\rightarrow Z=1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rsTBv8cN-5t",
        "cellView": "form"
      },
      "source": [
        "#@title Visualizing logistic regression model\n",
        "\n",
        "#Function to visualize logistic regression model\n",
        "def modelplot(beta,X,y):\n",
        "    if X.shape[1]==1: #Procedure for data with one variable\n",
        "        #Create the location of X to visualize\n",
        "        X_plot = np.linspace(X.min()-0.1,X.max()+0.1,51)\n",
        "        #Calculates prediction at prediction locations\n",
        "        F = np.ones((51,2))\n",
        "        F[:,1] = X_plot\n",
        "        _, y_pred = predict(X, beta)\n",
        "\n",
        "        fig = plt.figure(figsize=[8,6])\n",
        "        plt.scatter(X,y,label='Samples')\n",
        "        plt.plot(X_plot,y_pred,color='black',label='Logistic Regression Model')\n",
        "        plt.title('Logistic Regression Model on The Samples',fontsize=16)\n",
        "        plt.xlabel('x',fontsize=14)\n",
        "        plt.ylabel('y',fontsize=14)\n",
        "        plt.legend()\n",
        "        plt.show\n",
        "    elif X.shape[1] == 2: #Procedure for data with two variables\n",
        "        \n",
        "        #Create the location of X to visualize\n",
        "        x_min, x_max = X[:,0].min()-0.1, X[:,0].max()+0.1\n",
        "        y_min, y_max = X[:,1].min()-0.1, X[:,1].max()+0.1\n",
        "        h = .25\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "        #Calculate prediction at each prediction location\n",
        "        _,Z = predict(np.c_[xx.ravel(), yy.ravel()], beta)\n",
        "        cond_list = [Z<0.5,Z>=0.5]\n",
        "        choice_list = [0, 1]\n",
        "        Z = np.select(cond_list,choice_list)\n",
        "\n",
        "        # Put the result into a color plot\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        plt.figure(1, figsize=(8, 6))\n",
        "        plt.set_cmap(plt.cm.Paired)\n",
        "        plt.pcolormesh(xx, yy, Z)\n",
        "        pos = np.where(y==1) #To find which row(s) that have y=1\n",
        "        neg = np.where(y==0) #To find which row(s) that have y=0\n",
        "        plt.scatter(X[pos,0],X[pos,1],label='y=1',c='tab:brown',edgecolors='k')\n",
        "        plt.scatter(X[neg,0],X[neg,1],label='y=0',c='tab:cyan',edgecolors='k')\n",
        "        plt.xlabel('x1', fontsize=14)\n",
        "        plt.ylabel('x2', fontsize=14)\n",
        "        plt.title('Logistic Regression Model on The Samples',fontsize=16)\n",
        "        plt.legend(fontsize=14)\n",
        "        plt.show()\n",
        "        \n",
        "    else:\n",
        "        print(\"Your data has more than two variables\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwpeZeuGOHnT"
      },
      "source": [
        "#To visualize the logistic regression model, call modelplot function\n",
        "modelplot(beta,X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coNli-_D0_u7"
      },
      "source": [
        "# K-Nearest Neighbor Classification Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8SvrFtFQadO"
      },
      "source": [
        "To clarify the concept of a **parametric model**, it is a model in which the **number of parameters are fixed**, implying that the model can only take on a single form/function. For example, a **linear regression** model takes on the form:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y} = \\hat{\\beta_0} + \\sum_{i=1}^{p} \\hat{\\beta_p} X_{p}\n",
        "\\end{equation}\n",
        "\n",
        "This means that **no matter what your data looks like, your model will always produce a linear function to model the data.** \n",
        "\n",
        "Likewise, with logistic regression it takes on the form,\n",
        "\n",
        "$$p(X)=  \\frac{L}{1+e^{-(\\beta X)} }$$\n",
        "\n",
        "Due to the $\\beta X$ term in the exponent, this yields the logistic regression model as a **linear classifier**. Meaning, with any data is given to the model, it will always produce a linear line that separates the different classes specified in the data.  **Note: This linear line is called a hyperplane in the context of classification**.\n",
        "\n",
        "Parametric models are useful given we know the nature of the data, e.g. linear, polynomial, etc. However, what if the data in which no prior information about its nature is known? This is where **non-parametric** models '*enters the chat*'. With non-parametric models, the number of parameters is not fixed. In that way, a non-parametric model can take on different forms, reflective of the data. In this course, we introduce the simplest non-parametric model, that is, the **KNN-classifier**.\n",
        "\n",
        "We will use the KNN-classifier from the popular machine learning package, i.e., ```sklearn```. Notice that the program can handle multidimensional and multi classes data. Thus, although we demonstrate its working principle on two-variable and two-classes data, you can use it for more complex problems!\n",
        "\n",
        "Let us begin by executing the cell below (if the data is either one- or two-dimensional, the code will also plot your data):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj_170BQ6mDM",
        "cellView": "form"
      },
      "source": [
        "#@title Visualization of KNN classification\n",
        "from sklearn import neighbors #Import sklearn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def modelplotKNN(knn,X,y):\n",
        "  if X.shape[1]==1: #Procedure for data with one variable\n",
        "        #Create the location of X to visualize\n",
        "        X_plot = np.linspace(X.min()-0.1,X.max()+0.1,51)\n",
        "        #Calculate prediction at each prediction location\n",
        "        y_plot = knn.predict(X_plot)\n",
        "\n",
        "        #Create plot\n",
        "        fig = plt.figure(figsize=[8,6])\n",
        "        plt.scatter(X,y,label='Samples')\n",
        "        plt.plot(X_plot,y_plot,color='black',label='Logistic Regression Model')\n",
        "        plt.title('Logistic Regression Model on The Samples',fontsize=16)\n",
        "        plt.xlabel('x',fontsize=14)\n",
        "        plt.ylabel('y',fontsize=14)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "  elif X.shape[1] == 2: #Procedure for data with two variables\n",
        "        #Create the location of X to visualize\n",
        "        x_min, x_max = X[:,0].min()-0.1, X[:,0].max()+0.1\n",
        "        y_min, y_max = X[:,0].min()+0.1, X[:,1].max()+0.1\n",
        "        \n",
        "        hx = (x_max-x_min)/250\n",
        "        hy = (y_max-y_min)/250\n",
        "        #Calculate prediction at each prediction location\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
        "        X_norm = np.c_[xx.ravel(), yy.ravel()]\n",
        "        #Scale X for KNN model\n",
        "        scaler_X = min_max_scaler.fit(X)\n",
        "        X_norm = scaler_X.transform(X_norm)\n",
        "        Z = knn.predict(X_norm)\n",
        "\n",
        "        # Put the result into a color plot\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        plt.figure(1, figsize=(8, 6))\n",
        "        plt.set_cmap(plt.cm.Paired)\n",
        "        plt.pcolormesh(xx, yy, Z)\n",
        "        pos = np.where(y==1) #To find which row(s) that have y=1\n",
        "        neg = np.where(y==0) #To find which row(s) that have y=0\n",
        "        plt.scatter(X[pos,0],X[pos,1],label='y=1',c='tab:brown',edgecolors='k')\n",
        "        plt.scatter(X[neg,0],X[neg,1],label='y=0',c='tab:cyan',edgecolors='k')\n",
        "        plt.xlabel('x1', fontsize=14)\n",
        "        plt.ylabel('x2', fontsize=14)\n",
        "        plt.title('Logistic Regression Model on The Samples',fontsize=16)\n",
        "        plt.legend(fontsize=14)\n",
        "        plt.show()\n",
        "  else:\n",
        "        print(\"Your data has more than two variables\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Ny1dhjmjvI"
      },
      "source": [
        "Are you done? Now let's create a KNN-classifier prediction by varying the $k$ (i.e., number of neighbors) value. You can do just that by changing the value of ```n_neighbors``` (which is essentially $k$) below and then run the cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttS5Ljtw622t"
      },
      "source": [
        "#Define the number of neighbors for kNN classification\n",
        "n_neighbors = 10\n",
        "#Initiate min_max_scaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "#Scale X using min_max_scaler\n",
        "X_norm = min_max_scaler.fit_transform(X)\n",
        "#Initiate kNN classification model\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors)\n",
        "#Train kNN classification model on the data\n",
        "knn.fit(X_norm,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRdhbvnmXPc"
      },
      "source": [
        "Now let's visualize the resulting KNN prediction on your data set (**this works only for one- or two-dimensional problem with two classes**):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tesO0yzzmYkA"
      },
      "source": [
        "#Visualize the kNN classification model\n",
        "modelplotKNN(knn, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can make predictions anywhere. Look at the following cell to see one example:"
      ],
      "metadata": {
        "id": "N6YSpLNgVd0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_pred = np.array([[60,40],[60,70]]) # prediction sites, test at x = [60,40], and x = [60, 70]\n",
        "X_pred_norm = min_max_scaler.transform(X_pred) # Normalize to 0-1\n",
        "y_pred_knn = knn.predict(X_pred_norm) # Predict\n",
        "\n",
        "print(y_pred_knn) # Print the result"
      ],
      "metadata": {
        "id": "ktgEjPtwGxNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXZy94Fibmgq"
      },
      "source": [
        "As you can see, unlike the Logistic Regression hyperplane, the KNN hyperplane is non linear, and the one in the above image may also not be the best hyperplane, implying that there are other configurations in which the model can take. This is how a non-parametric model would behave."
      ]
    }
  ]
}